import GgTrends from "../assets/jsx_of_svg/gg_trends";
import DfiaSites from "../assets/jsx_of_svg/dfia_sites";

// https://dl.acm.org/doi/pdf/10.1145/3613904.3642382
// ^ may-june 2023

// // Aggregated across countries,
// 2.2% (95% CI = 1.8% − 2.6%) of respondents reported some form of victimization. This included:
// 1.2% (95% CI = 0.9% − 1.5%) of respondents who reported that someone had created deepfake pornography content of them;
// 1.3% (95% CI = 1.0% − 1.6%) who reported that someone had posted or sent deepfake pornography content of them; and
// 1.2% (95% CI = 0.9% − 1.5%) who reported that someone had threatened to post or send deepfake pornography content of them.

// Aggregating across behavior types (creating, posting, threatening to post), self-reported deepfake pornography perpetration was rare (
// 1.8%, 95% CI = 1.4% − 2.2%)
// Aggregating across countries,
// 1.0% (95% CI = 0.8%−1.4%) of respondents indicated creating deepfake pornography,
// 1.0% (95% CI = 0.8% − 1.3%) reported threatening to post, send, or share deepfake pornography, and
// 0.7% (95% CI = 0.5% − 0.9%) reported actually posting, sending, or sharing deepfake pornography content.

function Mission() {
  return (
    <div>
      In mid-2023, an academic survey of over 16,000 participants across 10 countries found that 2.2% of participants were victim-survivors of deepfake image-based sexual abuse. 1.8% of participants admitted to perpetrating this sort of abuse. However, these numbers were taken before the AI bubble had begun to hit. The numbers today are not available - yet another product of the AI overwhelm, but a critical one for understanding the amount of harm that this type of sexual abuse is wreaking globally.
      <button>link to study</button>

      <button>click here for graph</button>
      {/* <GgTrends/> */}
      {/* <div
        id="site_graph_container"
        style={{
          objectFit: "contain",
          height: "60vh",
          width: "auto",
          backgroundColor: "blue",
        }}
      >
        <DfiaSites />
      </div> */}
    </div>
  );
}

export default Mission;

{
  /* <div> */
}
{
  /* <div>
          <p>Headlines</p>
        </div>
        <div>
          <h2>
            Despite the lack of headlines, 'deepfake porn' has not gone away.
          </h2>
          <h3>
            We have been monitoring it using a systematic and rigorous academic
            methodology since 2021. Once created, the content is endlessly
            redistributed across the internet, appearing eventually on 'regular'
            websites...
          </h3>
          victim harms...
          scale: */
}

{
  /* <div style={{ maxHeight: "30vh", width: "30vw" }}>
            <Gg_trends />
          </div> */
}
{
  /* </div> */
}
{
  /* </div> */
}
{
  /* <div>
        DeepfAIke "porn" (DFIAM) is a type of image-based sexual abuse material
        (DFIAM), separated from child sexual abuse material (CSAM) only by the
        (apparent) age of the victim-survivor depicted. Distribution of DFIAM is
        illegal in ___ countries, including ___. Creation of DFIAM is illegal in
        ____. Despite this, there were 1,000 websites distributing DFIAM as of
        June 2025, and 1,000 websites where visitors can create DFIAM. 500 of
        these websites allow the visitor to create DFIAM of a real individual by
        uploading a single photo of the person they want to "nudify". As of
        2025, these websites were receiving ___ visits per month. at
        DefyAbuse.org we are the only entity monitoring the proliferation of
        DFIAM. We are doing our best to tackle it: outside of annually
        collecting and combing through thousands of websites (at minimum 18,000
        urls per year), we have advised the home office vawg team and the dist
        etc
      </div> */
}
